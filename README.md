# Skills Analysis Project Report

## Overview
This project involved analyzing a dataset of 50,000 job descriptions to uncover insights into skill demands and distributions across different roles. By implementing data processing and analysis using PySpark on Databricks, I developed proficiency in big data tools.

## Data Source
The Skill2Vec dataset compiled job postings data with detailed information on required skills. The O*NET dataset provided additional metadata on skills, technologies, and job codes.

## Data Processing
- Imported and configured PySpark and SQL modules for data manipulation.
- Loaded datasets into Spark dataframes for distributed processing.
- Performed cleaning and preprocessing like handling null values and normalizing text.
- Joined datasets and transformed data into formats for analysis.

## Analysis
- Aggregated data to calculate skill frequencies across job roles.
- Identified most common and in-demand skills requested by employers.
- Analyzed distributions of skill counts required for different positions.
- Categorized skills into topics and subgroups for deeper insights.
- Compared datasets to determine overlap in key skill demands.

## Tools and Techniques
- PySpark for data engineering on big datasets
- DataFrames, RDDs, and SQL for distributed data processing
- Statistical analysis and aggregations for quantitative insights
- Data visualizations like plots and charts
- Distributed computing capabilities of Databricks
